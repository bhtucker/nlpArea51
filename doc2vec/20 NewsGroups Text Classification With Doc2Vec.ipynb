{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from abstract import *\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism','rec.sport.baseball','talk.politics.mideast','comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#pprint(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def preprocessor(text, stem=False, stop=False, sent=False):\n",
    "    \"\"\"\n",
    "    Tokenize :text use nltk.word_tokenize, optionally:\n",
    "        stemming tokens if :stem\n",
    "        removeing stop words if :stop\n",
    "        joining tokens into one string with whitespace if :sent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove punctuations\n",
    "    exclude = set(string.punctuation)\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    " \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if stop:\n",
    "        stop = stopwords.words('english')\n",
    "        tokens =[word for word in tokens if word not in stop]\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    if sent:\n",
    "        tokens = ' '.join(tokens)\n",
    "        \n",
    "    return tokens\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'forgiv ani inaccuraci i delet the origin post isnt thi the same person who wrote the book and wa censur in canada a few year back'\n"
     ]
    }
   ],
   "source": [
    "pprint(preprocessor(newsgroups_train.data[0], sent=True, stem=True))\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "vectorizer = TfidfVectorizer(tokenizer=partial(preprocessor))  # stem=True, stop=True\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Training \n",
    "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "labels_train = le.fit_transform(newsgroups_train.target)\n",
    "\n",
    "# Just for debug quickly reassign labels\n",
    "X = vectors_train\n",
    "y = labels_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.02      0.04       145\n",
      "          1       1.00      0.03      0.05       213\n",
      "          2       0.21      1.00      0.35       186\n",
      "          3       1.00      0.02      0.04       205\n",
      "          4       1.00      0.18      0.31       191\n",
      "\n",
      "avg / total       0.84      0.25      0.16       940\n",
      "\n",
      "[[  3   0   0   0   0]\n",
      " [  0   6   0   0   0]\n",
      " [142 207 186 201 156]\n",
      " [  0   0   0   4   0]\n",
      " [  0   0   0   0  35]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#clf = MultinomialNB(alpha=.01)\n",
    "clf = svm.SVC(kernel='linear', C=0.05)\n",
    "\n",
    "# Evaluate using cross validation\n",
    "cross_val = KFold(len(newsgroups_train.target), n_folds=3, shuffle=True)\n",
    "\n",
    "for train_index, test_index in cross_val:\n",
    "    clf = svm.SVC(kernel='linear', C=0.05)\n",
    "    X_train, X_test = X[train_index],X[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print classification_report(le.inverse_transform(y_test),le.inverse_transform(y_pred))\n",
    "    print metrics.confusion_matrix(y_pred,y_test)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.72      0.71       319\n",
      "          1       0.87      0.85      0.86       389\n",
      "          2       0.80      0.87      0.84       397\n",
      "          3       0.78      0.81      0.79       394\n",
      "          4       0.88      0.76      0.82       376\n",
      "\n",
      "avg / total       0.81      0.81      0.81      1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1.)\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "print classification_report(le.inverse_transform(newsgroups_test.target),le.inverse_transform(pred))\n",
    "# stem and stop 0.82 (.78 - .86)\n",
    "# no stem or stop .81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preprocessing for Doc2Vec \n",
    "\n",
    "newsgroups_train_tokens = map(partial(preprocessor, stop=True), newsgroups_train.data)\n",
    "newsgroups_test_tokens = map(partial(preprocessor, stop=True), newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import sys\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import nltk\n",
    "from gensim.models.doc2vec import LabeledSentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 doc sets: 2818 train, 1875 test\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import nltk\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "\n",
    "def convert_newsgroup(docs, split):\n",
    "    \"\"\"\n",
    "    Generate doc2vec.LabeledSentence instances from dataset :docs\n",
    "    with train/test indicator :split\n",
    "    \"\"\"\n",
    "    for i,v in enumerate(docs):\n",
    "        label = '%s_%s' % (split, i)\n",
    "        yield LabeledSentence(v, [label])\n",
    "\n",
    "test_docs = list(convert_newsgroup(newsgroups_test_tokens, 'test'))\n",
    "train_docs = list(convert_newsgroup(newsgroups_train_tokens, 'train'))\n",
    "\n",
    "all_newsgroup_documents = [train_docs, test_docs]\n",
    "\n",
    "doc_list = all_newsgroup_documents[:]  # for reshuffling per pass\n",
    "\n",
    "print('%d doc sets: %d train, %d test' % (len(doc_list), len(train_docs), len(test_docs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Doc2Vec(dm=0, dm_concat=1, size=300, window=5, negative=5, hs=0, min_count=3, workers=cores)\n",
    "#Doc2Vec(dm=0, dm_mean=1, size=300, window=5, negative=5, hs=0, min_count=3, workers=cores),\n",
    "\n",
    "dbow_model = Doc2Vec(dm=0, dm_concat=0, sample=1e-5, size=288, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "# dm_model =  Doc2Vec(dm=1, dm_mean=1, sample=1e-5, size=300, window=10, negative=5, hs=0, min_count=2, workers=cores)\n",
    "\n",
    "# TODO speed setup by sharing results of 1st model's vocabulary scan\n",
    "# dbow_model.load_word2vec_format('/home/skillachie/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "dbow_model.build_vocab(train_docs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "# dm_model.load_word2vec_format('/home/skillachie/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# dm_model.build_vocab(all_newsgroup_documents)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "\n",
    "\n",
    "# Models to evaluate\n",
    "#simple_models = [\n",
    " \n",
    "    # PV-DBOW  0.86 with Stem & hs=0\n",
    "    #Doc2Vec(dm=0, dm_concat=1,sample=1e-5, size=300, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    \n",
    "    #\n",
    "    #Doc2Vec(dm=0, dm_mean=1, sample=1e-5,size=300, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    \n",
    "    \n",
    "    #Doc2Vec(dm=0, size=300, negative=5, hs=0, min_count=1, workers=cores),\n",
    "    \n",
    "    # PV-DM w/average No good 0.84\n",
    "    #Doc2Vec(dm=1, dm_mean=1, sample=1e-5, size=300, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    \n",
    "    # PV-DM w/sum\n",
    "    #Doc2Vec(dm=1, dm_mean=0, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "#]\n",
    "\n",
    "# speed setup by sharing results of 1st model's vocabulary scan\n",
    "#simple_models[0].load_word2vec_format('/home/skillachie/Downloads/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#simple_models[0].build_vocab(all_newsgroup_documents)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "\n",
    "#print(simple_models[0])\n",
    "\n",
    "#for model in simple_models[1:]:\n",
    "    #model.reset_from(simple_models[0])\n",
    "#    model.load_word2vec_forma('/home/skillachie/nlpArea51/doc2vec/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#    model.build_vocab(all_newsgroup_documents)\n",
    "#    print(model)\n",
    "\n",
    "#models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "dbow_dmm_model = ConcatenatedDoc2Vec([dbow_model, dm_model])\n",
    "#models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is bulk training done with all the  docs from train, test,  and dev ? Compared to other methods training is only done on the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda: 1.)  # to selectively-print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_vectors(model, docs):\n",
    "    \"\"\"\n",
    "    Get Vectors From Word2Vec\n",
    "    \"\"\"\n",
    "    vectors_list = []\n",
    "    \n",
    "    for doc_no in range(len(docs)):\n",
    "        doc_label = docs[doc_no].tags[0]\n",
    "        doc_vector = model.docvecs[doc_label]\n",
    "        vectors_list.append(doc_vector)\n",
    "        \n",
    "    return vectors_list\n",
    "\n",
    "# TODO inferred vectors\n",
    "\n",
    "def get_infer_vectors(model, docs):\n",
    "    \n",
    "    vecs = []\n",
    "    for doc in docs:\n",
    "        vecs.append(model.infer_vector(doc.words))\n",
    "    return vecs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dbow_model.alpha, dbow_model.min_alpha = 0.025, 0.025\n",
    "# dbow_model.train(train_docs)\n",
    "?dbow_model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025\n",
      "638185\n",
      "0.0254512\n",
      "636462\n",
      "0.0259024\n",
      "638580\n",
      "0.0263536\n",
      "637404\n",
      "0.0268048\n",
      "637253\n",
      "0.027256\n",
      "637970\n",
      "0.0277072\n",
      "638143\n",
      "0.0281584\n",
      "637638\n",
      "0.0286096\n",
      "637298\n",
      "0.0290608\n",
      "637119\n",
      "0.029512\n",
      "637505\n",
      "0.0299632\n",
      "637583\n",
      "0.0304144\n",
      "636485\n",
      "0.0308656\n",
      "637135\n",
      "0.0313168\n",
      "637124\n",
      "0.031768\n",
      "636422\n",
      "0.0322192\n",
      "637863\n",
      "0.0326704\n",
      "636637\n",
      "0.0331216\n",
      "637577\n",
      "0.0335728\n",
      "637041\n",
      "0.034024\n",
      "637265\n",
      "0.0344752\n",
      "637536\n",
      "0.0349264\n",
      "637325\n",
      "0.0353776\n",
      "637330\n",
      "0.0358288\n",
      "636912\n",
      "0.03628\n",
      "637715\n",
      "0.0367312\n",
      "637049\n",
      "0.0371824\n",
      "637141\n",
      "0.0376336\n",
      "638616\n",
      "0.0380848\n",
      "637745\n",
      "0.038536\n",
      "637273\n",
      "0.0389872\n",
      "636911\n",
      "0.0394384\n",
      "636937\n",
      "0.0398896\n",
      "637741\n",
      "0.0403408\n",
      "637115\n",
      "0.040792\n",
      "637771\n",
      "0.0412432\n",
      "637026\n",
      "0.0416944\n",
      "637000\n",
      "0.0421456\n",
      "637824\n",
      "0.0425968\n",
      "637803\n",
      "0.043048\n",
      "637627\n",
      "0.0434992\n",
      "637283\n",
      "0.0439504\n",
      "638016\n",
      "0.0444016\n",
      "637662\n",
      "0.0448528\n",
      "637804\n",
      "0.045304\n",
      "637704\n",
      "0.0457552\n",
      "637236\n",
      "0.0462064\n",
      "637016\n",
      "0.0466576\n",
      "637395\n",
      "0.0471088\n",
      "636903\n",
      "0.025\n",
      "637537\n",
      "0.0254512\n",
      "637813\n",
      "0.0259024\n",
      "637340\n",
      "0.0263536\n",
      "637258\n",
      "0.0268048\n",
      "637152\n",
      "0.027256\n",
      "637140\n",
      "0.0277072\n",
      "636351\n",
      "0.0281584\n",
      "637591\n",
      "0.0286096\n",
      "637227\n",
      "0.0290608\n",
      "636811\n",
      "0.029512\n",
      "636838\n",
      "0.0299632\n",
      "637259\n",
      "0.0304144\n",
      "637531\n",
      "0.0308656\n",
      "637138\n",
      "0.0313168\n",
      "636851\n",
      "0.031768\n",
      "638267\n",
      "0.0322192\n",
      "637491\n",
      "0.0326704\n",
      "637487\n",
      "0.0331216\n",
      "637721\n",
      "0.0335728\n",
      "637771\n",
      "0.034024\n",
      "637592\n",
      "0.0344752\n",
      "636876\n",
      "0.0349264\n",
      "637636\n",
      "0.0353776\n",
      "637316\n",
      "0.0358288\n",
      "636787\n",
      "0.03628\n",
      "638007\n",
      "0.0367312\n",
      "637221\n",
      "0.0371824\n",
      "636869\n",
      "0.0376336\n",
      "637414\n",
      "0.0380848\n",
      "637924\n",
      "0.038536\n",
      "637778\n",
      "0.0389872\n",
      "637662\n",
      "0.0394384\n",
      "636882\n",
      "0.0398896\n",
      "637953\n",
      "0.0403408\n",
      "637525\n",
      "0.040792\n",
      "637528\n",
      "0.0412432\n",
      "637239\n",
      "0.0416944\n",
      "637237\n",
      "0.0421456\n",
      "636852\n",
      "0.0425968\n",
      "637484\n",
      "0.043048\n",
      "637685\n",
      "0.0434992\n",
      "637795\n",
      "0.0439504\n",
      "638198\n",
      "0.0444016\n",
      "636807\n",
      "0.0448528\n",
      "636857\n",
      "0.045304\n",
      "637855\n",
      "0.0457552\n",
      "637565\n",
      "0.0462064\n",
      "637419\n",
      "0.0466576\n",
      "637052\n",
      "0.0471088\n",
      "637165\n",
      "0.025\n",
      "637595\n",
      "0.0254512\n",
      "637589\n",
      "0.0259024\n",
      "637204\n",
      "0.0263536\n",
      "638084\n",
      "0.0268048\n",
      "637350\n",
      "0.027256\n",
      "636877\n",
      "0.0277072\n",
      "637167\n",
      "0.0281584\n",
      "637453\n",
      "0.0286096\n",
      "637391\n",
      "0.0290608\n",
      "637241\n",
      "0.029512\n",
      "638204\n",
      "0.0299632\n",
      "637023\n",
      "0.0304144\n",
      "637295\n",
      "0.0308656\n",
      "636585\n",
      "0.0313168\n",
      "637191\n",
      "0.031768\n",
      "638123\n",
      "0.0322192\n",
      "636659\n",
      "0.0326704\n",
      "636934\n",
      "0.0331216\n",
      "637716\n",
      "0.0335728\n",
      "637155\n",
      "0.034024\n",
      "636837\n",
      "0.0344752\n",
      "637118\n",
      "0.0349264\n",
      "637657\n",
      "0.0353776\n",
      "638155\n",
      "0.0358288\n",
      "636599\n",
      "0.03628\n",
      "637585\n",
      "0.0367312\n",
      "636822\n",
      "0.0371824\n",
      "638010\n",
      "0.0376336\n",
      "638559\n",
      "0.0380848\n",
      "638006\n",
      "0.038536\n",
      "637799\n",
      "0.0389872\n",
      "637354\n",
      "0.0394384\n",
      "637129\n",
      "0.0398896\n",
      "637046\n",
      "0.0403408\n",
      "636980\n",
      "0.040792\n",
      "637613\n",
      "0.0412432\n",
      "637616\n",
      "0.0416944\n",
      "637579\n",
      "0.0421456\n",
      "637402\n",
      "0.0425968\n",
      "637146\n",
      "0.043048\n",
      "636695\n",
      "0.0434992\n",
      "637399\n",
      "0.0439504\n",
      "638076\n",
      "0.0444016\n",
      "637378\n",
      "0.0448528\n",
      "637749\n",
      "0.045304\n",
      "637824\n",
      "0.0457552\n",
      "637017\n",
      "0.0462064\n",
      "637734\n",
      "0.0466576\n",
      "637032\n",
      "0.0471088\n",
      "637229\n",
      "0.025\n",
      "637641\n",
      "0.0254512\n",
      "637163\n",
      "0.0259024\n",
      "637649\n",
      "0.0263536\n",
      "637716\n",
      "0.0268048\n",
      "637327\n",
      "0.027256\n",
      "637174\n",
      "0.0277072\n",
      "637224\n",
      "0.0281584\n",
      "636829\n",
      "0.0286096\n",
      "638298\n",
      "0.0290608\n",
      "637490\n",
      "0.029512\n",
      "638866\n",
      "0.0299632\n",
      "637776\n",
      "0.0304144\n",
      "637176\n",
      "0.0308656\n",
      "637573\n",
      "0.0313168\n",
      "637450\n",
      "0.031768\n",
      "637037\n",
      "0.0322192\n",
      "637408\n",
      "0.0326704\n",
      "637585\n",
      "0.0331216\n",
      "637539\n",
      "0.0335728\n",
      "637685\n",
      "0.034024\n",
      "637429\n",
      "0.0344752\n",
      "637427\n",
      "0.0349264\n",
      "637293\n",
      "0.0353776\n",
      "637266\n",
      "0.0358288\n",
      "637707\n",
      "0.03628\n",
      "637267\n",
      "0.0367312\n",
      "637116\n",
      "0.0371824\n",
      "638665\n",
      "0.0376336\n",
      "637040\n",
      "0.0380848\n",
      "637531\n",
      "0.038536\n",
      "637357\n",
      "0.0389872\n",
      "636546\n",
      "0.0394384\n",
      "637522\n",
      "0.0398896\n",
      "637943\n",
      "0.0403408\n",
      "637642\n",
      "0.040792\n",
      "637842\n",
      "0.0412432\n",
      "638261\n",
      "0.0416944\n",
      "638092\n",
      "0.0421456\n",
      "637542\n",
      "0.0425968\n",
      "636887\n",
      "0.043048\n",
      "636447\n",
      "0.0434992\n",
      "637028\n",
      "0.0439504\n",
      "637636\n",
      "0.0444016\n",
      "637527\n",
      "0.0448528\n",
      "637760\n",
      "0.045304\n",
      "637529\n",
      "0.0457552\n",
      "637086\n",
      "0.0462064\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-88c1f6968b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#     dm_model.alpha, dm_model.min_alpha = alpha, alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhtucker/.virtualenvs/citadel/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_words, word_count, total_examples, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/Queue.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "# train_docs_ = list(train_docs)\n",
    "for gen in range(10):\n",
    "    alpha, min_alpha, passes = (0.025, 0.001, 50)\n",
    "    for epoch in range(passes):\n",
    "        shuffle(train_docs_)\n",
    "\n",
    "        #for name, train_model in models_by_name.items():\n",
    "\n",
    "        #Train\n",
    "        print alpha\n",
    "        dbow_model.alpha, dbow_model.min_alpha = alpha, alpha\n",
    "        print(dbow_model.train(train_docs_))\n",
    "\n",
    "    #     dm_model.alpha, dm_model.min_alpha = alpha, alpha\n",
    "    #     dm_model.train(doc_list)\n",
    "\n",
    "\n",
    "    #     dbow_dmm_model.alpha, dbow_dmm_model.min_alpha = alpha, alpha\n",
    "    #     dbow_dmm_model.train(doc_list)\n",
    "\n",
    "\n",
    "        alpha -= alpha_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hitter' in dbow_model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I'm not quite sure how these numbers are generated.  It appears that in\n",
      "a neutral park Bo's HR and slugging tend to drop (he actually loses two\n",
      "home runs).  Or do they?  What is \"equivalent average?\"\n",
      "\n",
      "One thing, when looking at Bo's stats, is that you can see that KC took\n",
      "away some homers.  Normally, you expect some would-be homers to go for\n",
      "doubles or triples in big parks, or to be caught, and for that matter you\n",
      "expect lots of doubles and triples anyway.  But Bo, despite his speed, \n",
      "hit very few doubles and not that many triples.  So I would expect his\n",
      "value to have risen quite considerably in a neutral park.  \n",
      "\n",
      "\n",
      "Felix Jose has been a .350/.440 player in a fairly neutral park.\n",
      "I would offhand guess the `89-`90 Bo at around a .330/.530 player.\n",
      "Maybe .330/.550 .  Not even close.\n",
      "\n",
      "\n",
      "I'd put him about there too.  \n",
      "\n",
      "Note: I hadn't realized the media had hyped him so much.  I thought he\n",
      "was always viewed by them as a better football player, and only so-so \n",
      "at baseball.  He did only have one 30-hr, 100-rbi season, and KC wasn't\n",
      "winning.\n",
      "\n",
      "Note 2: I maybe have harped on this a bit in the past, but there is a\n",
      "mistake being made (by the SDCN's, as they are known, on this group)\n",
      "with respect to players like Bo and Deion and Lofton (and perhaps others).\n",
      "\n",
      "We find, that if you look at a large group of players, their past major\n",
      "and minor league numbers will predict their future numbers fairly well.\n",
      "Their are some caveats: the younger they are, the less good the prediction;\n",
      "the lower the minor league, the less good (I imagine), the more recent\n",
      "the player has left college ball, etc.\n",
      "\n",
      "Now of course, this prediction involves quite a bit of \"error.\"  Sometimes\n",
      "a player with poor MLE's (Dave Justice, the 1990 Ventura) becomes a star.\n",
      "Some hitters develop (Shane Mack, Brian Downing), some don't (Oddibe\n",
      "McDowell, Mickey Brantley).  This error involves real things: there are\n",
      "real reasons why Oddibe didn't hit and Shane did.  It may (who knows)\n",
      "involve parks and batting coaches and wheaties and injuries and lifting\n",
      "and so on.\n",
      "\n",
      "But still, you have this big pool of players, and things work pretty well.\n",
      "One of the reasons for these predictions accuracy is the common background\n",
      "of the players.  One thing we know about professional baseball players is\n",
      "that all of them (or almost all) have spent a good deal of time playing\n",
      "ball.  Their backgrounds are similar.\n",
      "\n",
      "What hasn't been established is what happens when you encounter a player\n",
      "with a different background?  Is there some reason to believe that a\n",
      "Bo, or a Deion, or a Lofton, or a Tony Gwynn (?), or an Ainge, or so\n",
      "on, has such a different background, that the standard model and standard\n",
      "assumptions fit this person slowly?\n",
      "\n",
      "It hasn't been established that you can use MLE's with two-sport players.\n",
      "(It hasn't been established that you can't, but then statistics is, after\n",
      "all, an art).  I personally think otherwise lucid individuals continually\n",
      "make completely nonsensical statements about Bo and Deion and Lofton.\n",
      "\"Look at those good-but-not-great minor league numbers,\" they say.  Well,\n",
      "what happens if those numbers simply don't mean what they usually mean?\n",
      "It might mean that Ken Lofton suddenly has a better year in Houston than\n",
      "Tuscon.  It might mean that Deion suddenly has a better half-year in\n",
      "Atlanta than Greenville.  \n",
      "\n",
      "Then again, it might not.  Ken and Deion might go right back in the tank\n",
      "this year, live up to those poor MLE's.  But you guys DON'T KNOW.  What's \n",
      "worse, you don't know that you don't.  And you don't know that there are \n",
      "other players you won't know about -- injuries and lifting and wheaties \n",
      "again.  You seem to think that the model is perfect and eternal.  It's not.\n",
      "It's got some error.\n",
      "\n",
      "Oh well.\n",
      "\n",
      "Bill Guilford\n"
     ]
    }
   ],
   "source": [
    "for d in newsgroups_train.data:\n",
    "    if 'baseball' in d:\n",
    "        print d\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "god belief graphic reason\n",
      "graphic\n",
      "graphic\n",
      "pitch hitter clutch heretic\n",
      "clutch\n",
      "clutch\n",
      "congress hardware vote conservative\n",
      "congress\n",
      "congress\n",
      "shuttle launch fishing solar\n",
      "fishing\n",
      "fishing\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "god belief graphic reason\n",
    "graphic\n",
    "graphic\n",
    "pitch hitter clutch heretic\n",
    "clutch\n",
    "clutch\n",
    "congress hardware vote conservative\n",
    "congress\n",
    "congress\n",
    "shuttle launch fishing solar\n",
    "fishing\n",
    "fishing\n",
    "\"\"\"\n",
    "\n",
    "# categories = ['alt.atheism','rec.sport.baseball','talk.politics.mideast','comp.graphics', 'sci.space']\n",
    "untrained_model = Doc2Vec(dm=0, dm_concat=0, sample=1e-5, size=48, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "untrained_model.build_vocab(train_docs)\n",
    "for sample in [\n",
    "    'god belief graphic reason',\n",
    "    'pitch hitter clutch heretic',\n",
    "    'congress hardware vote conservative',\n",
    "    'shuttle launch fishing solar'\n",
    "]:\n",
    "    print(sample)\n",
    "    print(untrained_model.doesnt_match(sample.split()))\n",
    "    print(dbow_model.doesnt_match(sample.split()))    \n",
    "# dbow_model.doesnt_match('god belief graphic reason'.split())\n",
    "# newsgroups_train.filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       319\n",
      "          1       0.00      0.00      0.00       389\n",
      "          2       0.21      1.00      0.35       397\n",
      "          3       0.00      0.00      0.00       394\n",
      "          4       0.00      0.00      0.00       376\n",
      "\n",
      "avg / total       0.04      0.21      0.07      1875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "train_vectors = extract_vectors(untrained_model, train_docs)\n",
    "test_vectors = get_infer_vectors(untrained_model, test_docs)\n",
    "\n",
    "\n",
    "#model = LinearSVC()\n",
    "#penalties = np.array([0.001,0.002,0.003,0.004,0.005,0.007,0.008,0.009,0.01,0.05,0.04,0.03,0.02])\n",
    "#grid = GridSearchCV(estimator=model ,n_jobs=7,param_grid=dict(C=penalties))\n",
    "#grid.fit(train_vectors, newsgroups_train.target)\n",
    "        \n",
    "# summarize the results of the grid search\n",
    "#print(grid.best_score_)\n",
    "#print(grid.best_estimator_.C)\n",
    "\n",
    "#clf = LinearSVC(C=0.009)\n",
    "clf = LinearSVC(C=0.0025)\n",
    "clf.fit(train_vectors, newsgroups_train.target)\n",
    "\n",
    "predDoc = clf.predict(test_vectors)\n",
    "        \n",
    "print classification_report(le.inverse_transform(newsgroups_test.target), le.inverse_transform(predDoc))\n",
    "\n",
    "# avg / total       0.77      0.76      0.76      1875\n",
    "# avg / total       0.78      0.76      0.76      1875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.85      0.86       480\n",
      "          1       0.93      0.92      0.93       584\n",
      "          2       0.92      0.91      0.91       597\n",
      "          3       0.94      0.87      0.90       593\n",
      "          4       0.81      0.91      0.85       564\n",
      "\n",
      "avg / total       0.90      0.89      0.89      2818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(le.inverse_transform(newsgroups_train.target), le.inverse_transform(clf.predict(train_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
